{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74979aa8",
   "metadata": {},
   "source": [
    "# PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efda79cc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 08:24:14.194428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-14 08:24:15.021762: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-14 08:24:15.022818: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-14 08:24:15.048241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-02-14 08:24:15.049005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-02-14 08:24:15.049698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:86:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-02-14 08:24:15.050215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
      "pciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-02-14 08:24:15.050239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-14 08:24:15.051938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-14 08:24:15.051988: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-14 08:24:15.053663: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-14 08:24:15.053930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-14 08:24:15.055707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-14 08:24:15.056618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-14 08:24:15.060396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-14 08:24:15.064989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    multilabel_confusion_matrix,\n",
    "    ConfusionMatrixDisplay, \n",
    "    accuracy_score\n",
    ")\n",
    "from wildlifeml.utils.io import (\n",
    "    load_csv, \n",
    "    load_json, \n",
    "    load_pickle,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa570de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REPODIR = '/home/lisa-wm/Documents/1_work/1_research/repos/wildlife-experiments'\n",
    "REPODIR = '/home/wimmerl/projects/wildlife-experiments'\n",
    "\n",
    "CFG = load_json(os.path.join(REPODIR, 'configs/cfg.json'))\n",
    "LABEL_MAP = load_json(os.path.join(REPODIR, 'data/label_map.json'))\n",
    "EMPTY_CLASS = LABEL_MAP['empty']\n",
    "CONFMAT_ARGS = {\n",
    "    'normalize': 'true',\n",
    "    'values_format': '.2f',\n",
    "    'display_labels': list(LABEL_MAP.keys()),\n",
    "    'xticks_rotation': 'vertical',\n",
    "    'colorbar': False,\n",
    "    'cmap': 'Blues',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ae8cf",
   "metadata": {},
   "source": [
    "# IN-SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d66336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RESULTS_INSAMPLE_TEST = load_pickle('results/202302101741_insample_test.pkl')\n",
    "RESULTS_INSAMPLE_VAL = load_pickle('results/202302101741_insample_val.pkl')\n",
    "RESULTS_BUGFIX = load_pickle('results/202302131319_insample_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c0301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = RESULTS_BUGFIX\n",
    "THRESHOLDS = list(RESULT_FILE.keys())\n",
    "OUR_THRESH = 0.25\n",
    "PREDS = 'preds_imgs_clf'\n",
    "TRUTH = 'truth_imgs_clf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09249b7",
   "metadata": {},
   "source": [
    "## PERFORMANCE ACROSS THRESHOLDS ON TEST\n",
    "\n",
    "Using varying thresholds for the MD, how well is our pipeline able to 1) classify images overall, 2) detect empty images correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae3ad2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1187   44]\n",
      "  [  30  168]]]\n",
      "[[[1207    4]\n",
      "  [  87   52]]]\n",
      "[[[1176   29]\n",
      "  [  44   81]]]\n",
      "[[[1197    3]\n",
      "  [  81   30]]]\n",
      "[[[1167   21]\n",
      "  [  50   46]]]\n",
      "[[[1171   10]\n",
      "  [  53   30]]]\n"
     ]
    }
   ],
   "source": [
    "rows_multiclass = []\n",
    "rows_empty = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    result = RESULT_FILE[t]\n",
    "    y_true = result[TRUTH]\n",
    "    y_pred = [np.argmax(v) for v in result[PREDS].values()]\n",
    "    report = classification_report(\n",
    "        y_true=y_true, \n",
    "        y_pred=y_pred,\n",
    "        target_names=list(LABEL_MAP.keys()),\n",
    "        zero_division=0,\n",
    "        output_dict=True,\n",
    "    )\n",
    "    rows_multiclass.append(\n",
    "        [\n",
    "            t,\n",
    "            report['accuracy'], \n",
    "            report['weighted avg']['precision'], \n",
    "            report['weighted avg']['recall'], \n",
    "            report['weighted avg']['f1-score']\n",
    "        ]    \n",
    "    )\n",
    "    conf_empty = multilabel_confusion_matrix(y_true, y_pred, labels=[EMPTY_CLASS])\n",
    "    print(conf_empty)\n",
    "    rows_empty.append(\n",
    "        [\n",
    "            t,\n",
    "            (conf_empty[0][0, 0] + conf_empty[0][1, 1]) / conf_empty.sum(),\n",
    "            report['empty']['precision'],\n",
    "            report['empty']['recall'],\n",
    "            report['empty']['f1-score'],\n",
    "        ]\n",
    "    )\n",
    "df_ins_multiclass = pd.DataFrame(rows_multiclass, columns=['threshold', 'acc', 'prec', 'rec', 'fone'])\n",
    "df_ins_empty = pd.DataFrame(rows_empty, columns=['threshold', 'acc', 'prec', 'rec', 'fone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0f03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ins_multiclass  # Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bdf01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ins_empty  # Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684d7c5",
   "metadata": {},
   "source": [
    "## PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc877ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ours = RESULTS_INSAMPLE_TEST[THRESHOLDS[0]]\n",
    "y_true = result_ours['truth_imgs_ppl']\n",
    "y_pred = [np.argmax(v) for v in result_ours['preds_imgs_ppl'].values()]\n",
    "report = classification_report(\n",
    "    y_true=y_true, \n",
    "    y_pred=y_pred,\n",
    "    target_names=list(LABEL_MAP.keys())\n",
    ")\n",
    "print(report)  # Figure 3\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, **CONFMAT_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11fe27",
   "metadata": {},
   "source": [
    "# OUT-OF-SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c9861",
   "metadata": {},
   "source": [
    "## PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_OOSAMPLE = load_pickle('results/202302101741_oosample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebeeb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = RESULTS_OOSAMPLE['truth_imgs_ppl']\n",
    "y_pred = [np.argmax(v) for v in RESULTS_OOSAMPLE['preds_imgs_ppl'].values()]\n",
    "report_dict = classification_report(\n",
    "    y_true=y_true, \n",
    "    y_pred=y_pred,\n",
    "    target_names=list(LABEL_MAP.keys()),\n",
    "    output_dict=True,\n",
    ")\n",
    "rows_oos_multiclass = []\n",
    "rows_oos_multiclass.append(\n",
    "    [\n",
    "        report_dict['accuracy'], \n",
    "        report_dict['weighted avg']['precision'], \n",
    "        report_dict['weighted avg']['recall'], \n",
    "        report_dict['weighted avg']['f1-score']\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oos_multiclass = pd.DataFrame(\n",
    "    rows_oos_multiclass,\n",
    "    columns=['acc', 'prec', 'rec', 'fone']\n",
    ")\n",
    "df_multiclass = pd.concat([df_ins_multiclass.loc[df_ins_multiclass['threshold'] == OUR_THRESH], df_oos_multiclass])\n",
    "df_multiclass.index = ['ins', 'oos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b193d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiclass.drop(['threshold'], axis=1)  # Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(  # Figure 5\n",
    "    classification_report(\n",
    "        y_true=y_true, \n",
    "        y_pred=y_pred,\n",
    "        target_names=list(LABEL_MAP.keys())\n",
    "    )\n",
    "\n",
    ") \n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, **CONFMAT_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df1f9e",
   "metadata": {},
   "source": [
    "## ACTIVE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ACTIVE_UPPER_BASELINE = load_pickle('results/202302101959_results_active_optimal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_true=RESULTS_ACTIVE_UPPER_BASELINE['truth_imgs_ppl'], \n",
    "        y_pred=[np.argmax(v) for v in RESULTS_ACTIVE_UPPER_BASELINE['preds_imgs_ppl'].values()],\n",
    "        target_names=list(LABEL_MAP.keys())\n",
    "    )\n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ffe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_upper_perf_limit = load_pickle(\n",
    "#     'results/202212011038_results_oosample_active_optimal.json'\n",
    "# )\n",
    "# eval_al_coldstart = load_pickle('results/202212041731_eval_logfile_coldstart.json')\n",
    "# eval_al_warmstart = load_pickle('results/202212080940_eval_logfile_warmstart.json')\n",
    "# # compute relative sample size per iteration\n",
    "# # (sizes hard-coded for training)\n",
    "# n_samples_total = len(load_pickle('data/dataset_oos_trainval.pkl').keys)\n",
    "# num_max_batches = (\n",
    "#     (n_samples_total - (5 * 128 + 5 * 256 + 5 * 512)) // 1024\n",
    "# )\n",
    "# size_last_batch = (\n",
    "#     n_samples_total - (5 * 128 + 5 * 256 + 5 * 512 + num_max_batches * 1024)\n",
    "# )\n",
    "# batch_sizes = (\n",
    "#     5 * [128] + 5 * [256] + 5 * [512] + num_max_batches * [1024]\n",
    "# )\n",
    "# if size_last_batch > 0:\n",
    "#     batch_sizes.append(size_last_batch)\n",
    "# batch_sizes_cs = np.cumsum([x / n_samples_total for x in batch_sizes])\n",
    "# # inspect upper performance limit\n",
    "# df_pred_upper_perf_limit = rf.build_df_pred(eval_upper_perf_limit, label_dict, detector_dict, image_data_dir)\n",
    "# df_pred_upper_perf_limit = rf.labelize_df_pred(df_pred_upper_perf_limit, label_map)\n",
    "# df_pred_upper_perf_limit = pd.merge(df_pred_upper_perf_limit, df_meta, on=['img_name', 'true_class'], how='inner')\n",
    "# report_upper_perf_limit = classification_report(\n",
    "#     y_true=df_pred_upper_perf_limit['true_class'],\n",
    "#     y_pred=df_pred_upper_perf_limit['pred_class'],\n",
    "#     labels=labels,\n",
    "#     zero_division=0,\n",
    "#     output_dict=False,\n",
    "# )\n",
    "# print(report_upper_perf_limit)\n",
    "\n",
    "# rf.inspect_confusion(\n",
    "#     df_pred=df_pred_upper_perf_limit,\n",
    "#     normalize=True,\n",
    "#     labels=labels,\n",
    "#     ax=None,\n",
    "# )\n",
    "# report_upper_perf_limit_dict = classification_report(\n",
    "#     y_true=df_pred_upper_perf_limit['true_class'],\n",
    "#     y_pred=df_pred_upper_perf_limit['pred_class'],\n",
    "#     labels=labels,\n",
    "#     zero_division=0,\n",
    "#     output_dict=True,\n",
    "# )\n",
    "# print(report_upper_perf_limit_dict)\n",
    "# # compute metrics per iteration\n",
    "\n",
    "# acc_coldstart = []\n",
    "# rec_coldstart = []\n",
    "# acc_warmstart = []\n",
    "# rec_warmstart = []\n",
    "\n",
    "# for iteration in eval_al_coldstart.keys():\n",
    "#     df_cs = rf.build_df_pred(eval_al_coldstart[iteration], label_dict, detector_dict, image_data_dir)\n",
    "#     df_cs = rf.labelize_df_pred(df_cs, label_map)\n",
    "#     df_cs = pd.merge(df_cs, df_meta, on=['img_name', 'true_class'], how='inner')\n",
    "#     report_cs = classification_report(\n",
    "#         y_true=df_cs['true_class'],\n",
    "#         y_pred=df_cs['pred_class'],\n",
    "#         labels=labels,\n",
    "#         zero_division=0,\n",
    "#         output_dict=True,\n",
    "#     )\n",
    "#     df_ws = rf.build_df_pred(eval_al_warmstart[iteration], label_dict, detector_dict, image_data_dir)\n",
    "#     df_ws = rf.labelize_df_pred(df_ws, label_map)\n",
    "#     df_ws = pd.merge(df_ws, df_meta, on=['img_name', 'true_class'], how='inner')\n",
    "#     report_ws = classification_report(\n",
    "#         y_true=df_ws['true_class'],\n",
    "#         y_pred=df_ws['pred_class'],\n",
    "#         labels=labels,\n",
    "#         zero_division=0,\n",
    "#         output_dict=True,\n",
    "#     )\n",
    "#     acc_coldstart.append(report_cs['accuracy'])\n",
    "#     rec_coldstart.append(report_cs['weighted avg']['recall'])\n",
    "#     acc_warmstart.append(report_ws['accuracy'])\n",
    "#     rec_warmstart.append(report_ws['weighted avg']['recall'])\n",
    "\n",
    "# # create data for plot\n",
    "# al_perf_acc = {\n",
    "#     'relative_size': batch_sizes_cs[1:],\n",
    "#     'acc_lower_limit': [report_lower_perf_limit_dict['accuracy'] for _ in range(len(acc_coldstart))],\n",
    "#     'acc_upper_limit': [report_upper_perf_limit_dict['accuracy'] for _ in range(len(acc_coldstart))],\n",
    "#     'acc_coldstart': acc_coldstart,\n",
    "#     'acc_warmstart': acc_warmstart\n",
    "# }\n",
    "# al_perf_acc_pd = pd.DataFrame(al_perf_acc)\n",
    "# al_perf_rec = {\n",
    "#     'relative_size': batch_sizes_cs[1:],\n",
    "#     'rec_lower_limit': [report_lower_perf_limit_dict['weighted avg']['recall'] for _ in range(len(acc_coldstart))],\n",
    "#     'rec_upper_limit': [report_upper_perf_limit_dict['weighted avg']['recall'] for _ in range(len(acc_coldstart))],\n",
    "#     'rec_coldstart': rec_coldstart,\n",
    "#     'rec_warmstart': rec_warmstart\n",
    "# }\n",
    "# al_perf_acc_pd = pd.DataFrame(al_perf_acc)\n",
    "# al_perf_rec_pd = pd.DataFrame(al_perf_rec)\n",
    "# plt = sns.lineplot(\n",
    "#     data=pd.melt(al_perf_acc_pd, id_vars='relative_size'),\n",
    "#     x='relative_size',\n",
    "#     y='value',\n",
    "#     hue='variable',\n",
    "#     style='variable'\n",
    "# )\n",
    "# plt.set(xlabel='relative sample size', ylabel='accuracy')\n",
    "# plt.legend(\n",
    "#     title='training mode', \n",
    "#     loc='lower right', \n",
    "#     labels=['lower baseline', 'upper baseline', 'cold start', 'warm start']\n",
    "# )\n",
    "# plt.figure.savefig('oos_active.eps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
